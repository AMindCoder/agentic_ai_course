{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Chaining Design Pattern: Order Enquiry Agent\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the **Prompt Chaining** pattern - a workflow where the output of one LLM call becomes the input to the next in a predetermined sequence. Unlike agents that dynamically decide their actions, prompt chains follow a fixed path with conditional branching.\n",
    "\n",
    "## Use Case\n",
    "An automated customer service agent that:\n",
    "1. Extracts order information from natural language queries\n",
    "2. Looks up order details in a database\n",
    "3. **Validates** the lookup result and handles errors\n",
    "4. Retrieves current shipping status (if order found)\n",
    "5. Generates a personalized response\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "User Query â†’ Extract Info â†’ Database Lookup â†’ Validation\n",
    "                                                  â”œâ”€ Not Found â†’ END (with error message)\n",
    "                                                  â””â”€ Found â†’ Check Status â†’ Generate Response â†’ END\n",
    "```\n",
    "\n",
    "### Key Pattern: Validation Node\n",
    "The **validation node** acts as a gateway that stops the flow when an order is not found, avoiding unnecessary processing and providing immediate error feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "Install required packages for building the prompt chaining workflow with LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph langchain-core langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries for state management, message handling, and LLM interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Optional, List, TypedDict\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# LangChain OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "print(\"âœ… Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "Configure OpenAI API credentials. For Google Colab, use userdata to securely store your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - Load API key from userdata\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    print(\"âœ… API key loaded from Google Colab userdata\")\n",
    "except ImportError:\n",
    "    # For local environment - set your API key directly\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "    print(\"âš ï¸  Running locally - Please set your API key\")\n",
    "\n",
    "# Initialize ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.2,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"âœ… ChatOpenAI model configured\")\n",
    "print(f\"   Model: {llm.model_name}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from JSON Files\n",
    "\n",
    "Load customer and order data from JSON files. The data includes complete information with shipping details, tracking numbers, and order history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from JSON files\n",
    "def load_data():\n",
    "    \"\"\"Load customers and orders from JSON files\"\"\"\n",
    "    data_dir = 'data'\n",
    "    \n",
    "    # Load customers\n",
    "    with open(f'{data_dir}/customers.json', 'r') as f:\n",
    "        customers_list = json.load(f)\n",
    "    \n",
    "    # Load orders\n",
    "    with open(f'{data_dir}/orders.json', 'r') as f:\n",
    "        orders_list = json.load(f)\n",
    "    \n",
    "    # Convert to dictionaries for fast lookup\n",
    "    customers_dict = {c['customer_id']: c for c in customers_list}\n",
    "    orders_dict = {o['order_id']: o for o in orders_list}\n",
    "    \n",
    "    return customers_dict, orders_dict\n",
    "\n",
    "# Load the data\n",
    "CUSTOMERS, ORDERS = load_data()\n",
    "\n",
    "print(\"âœ… Data loaded successfully from JSON files\")\n",
    "print(f\"ğŸ“Š Customers: {len(CUSTOMERS)}, Orders: {len(ORDERS)}\")\n",
    "print(f\"\\nğŸ“‹ Sample Customer IDs: {list(CUSTOMERS.keys())[:5]}\")\n",
    "print(f\"ğŸ“¦ Sample Order IDs: {list(ORDERS.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Definition\n",
    "\n",
    "Define the state schema that will be passed through each step of the prompt chain. Each node will read from and update this shared state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderEnquiryState(TypedDict):\n",
    "    \"\"\"State that flows through the prompt chain\"\"\"\n",
    "    user_query: str                          # Original user question\n",
    "    extracted_info: Optional[Dict]           # Step 1: Extracted order/customer info\n",
    "    order_data: Optional[Dict]               # Step 2: Retrieved order data\n",
    "    customer_data: Optional[Dict]            # Step 2: Retrieved customer data\n",
    "    order_found: bool                        # Routing decision\n",
    "    status_info: Optional[Dict]              # Step 3: Status and shipping info\n",
    "    final_response: str                      # Step 4: Generated response\n",
    "    \n",
    "print(\"âœ… State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Chain Step 1: Extract Information\n",
    "\n",
    "This node uses an LLM to extract structured information (order ID, customer details) from natural language queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_node(state: OrderEnquiryState) -> Dict:\n",
    "    \"\"\"\n",
    "    Step 1: Extract order ID and customer information from user query\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” STEP 1: Extracting order information...\")\n",
    "    \n",
    "    user_query = state[\"user_query\"]\n",
    "    \n",
    "    system_prompt = \"\"\"You are an information extraction assistant. \n",
    "Extract order ID and customer information from user queries.\n",
    "\n",
    "Return ONLY a JSON object with this format:\n",
    "{\n",
    "  \"order_id\": \"order number or null\",\n",
    "  \"customer_name\": \"customer name or null\",\n",
    "  \"customer_id\": \"customer ID or null\"\n",
    "}\n",
    "\n",
    "Examples:\n",
    "- \"Where is my order 101?\" â†’ {\"order_id\": \"101\", \"customer_name\": null, \"customer_id\": null}\n",
    "- \"I'm Sarah, checking order 101\" â†’ {\"order_id\": \"101\", \"customer_name\": \"Sarah\", \"customer_id\": null}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use LangChain ChatOpenAI with messages\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_query)\n",
    "        ]\n",
    "        \n",
    "        # Invoke the model with temperature=0 for extraction\n",
    "        extraction_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        response = extraction_llm.invoke(messages)\n",
    "        \n",
    "        extracted = json.loads(response.content)\n",
    "        print(f\"   Extracted: {json.dumps(extracted, indent=2)}\")\n",
    "        \n",
    "        return {\"extracted_info\": extracted}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "        return {\"extracted_info\": {\"order_id\": None, \"customer_name\": None, \"customer_id\": None}}\n",
    "\n",
    "print(\"âœ… Extract info node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prompt Chain Step 2: Database Lookup\n",
    "\n",
    "Query the database with extracted information to retrieve order and customer details. This step determines the routing path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_lookup_node(state: OrderEnquiryState) -> Dict:\n",
    "    \"\"\"\n",
    "    Step 2: Look up order in database\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ’¾ STEP 2: Querying database...\")\n",
    "    \n",
    "    extracted = state.get(\"extracted_info\", {})\n",
    "    order_id = extracted.get(\"order_id\")\n",
    "    \n",
    "    if not order_id:\n",
    "        print(\"   âŒ No order ID found in query\")\n",
    "        return {\n",
    "            \"order_data\": None,\n",
    "            \"customer_data\": None,\n",
    "            \"order_found\": False\n",
    "        }\n",
    "    \n",
    "    # Look up order\n",
    "    order_data = ORDERS.get(order_id)\n",
    "    \n",
    "    if not order_data:\n",
    "        print(f\"   âŒ Order {order_id} not found\")\n",
    "        return {\n",
    "            \"order_data\": None,\n",
    "            \"customer_data\": None,\n",
    "            \"order_found\": False\n",
    "        }\n",
    "    \n",
    "    # Look up customer\n",
    "    customer_data = CUSTOMERS.get(order_data[\"customer_id\"])\n",
    "    \n",
    "    print(f\"   âœ… Order {order_id} found for {customer_data['name']}\")\n",
    "    print(f\"   Status: {order_data['status']}\")\n",
    "    \n",
    "    return {\n",
    "        \"order_data\": order_data,\n",
    "        \"customer_data\": customer_data,\n",
    "        \"order_found\": True\n",
    "    }\n",
    "\n",
    "print(\"âœ… Database lookup node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Prompt Chain Step 3: Validation Node\n\nThis node validates whether the order was found after database lookup. If not found, it generates an error response and stops the flow. If found, the flow continues to check status."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validation_node(state: OrderEnquiryState) -> Dict:\n    \"\"\"\n    Step 3: Validate if order was found\n    If not found, generate error response and set flag to stop flow\n    \"\"\"\n    print(\"\\nâœ… STEP 3: Validating order lookup...\")\n    \n    order_found = state.get(\"order_found\", False)\n    \n    if not order_found:\n        # Generate error response when order not found\n        print(\"   âš ï¸  Order not found - Generating error response\")\n        \n        extracted = state.get(\"extracted_info\", {})\n        order_id = extracted.get(\"order_id\", \"unknown\")\n        \n        system_prompt = \"\"\"You are a helpful customer service representative.\nGenerate a polite error message for an order that wasn't found.\n\nGuidelines:\n- Be empathetic and apologetic\n- Suggest possible reasons (typo, wrong number, etc.)\n- Provide clear next steps\n- Include customer support contact: 1-800-SHOP-HELP\n- Keep it brief and friendly\n\"\"\"\n        \n        user_prompt = f\"Order ID '{order_id}' was not found in our system. Generate an appropriate response.\"\n        \n        try:\n            # Use LangChain ChatOpenAI\n            messages = [\n                SystemMessage(content=system_prompt),\n                HumanMessage(content=user_prompt)\n            ]\n            \n            response = llm.invoke(messages)\n            final_response = response.content\n            \n            print(f\"   âœ… Error response generated - Flow will stop\")\n            \n            return {\"final_response\": final_response}\n            \n        except Exception as e:\n            print(f\"   âŒ Error generating response: {e}\")\n            return {\n                \"final_response\": f\"I couldn't find order {order_id}. Please contact support at 1-800-SHOP-HELP.\"\n            }\n    else:\n        # Order found - continue to next step\n        print(\"   âœ… Order found - Continuing to status check\")\n        return {}\n\nprint(\"âœ… Validation node defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Prompt Chain Step 4: Check Status\n\nThis node retrieves the current order status and enriches it with tracking information and delivery details."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_status_node(state: OrderEnquiryState) -> Dict:\n    \"\"\"\n    Step 4: Retrieve and format current order status\n    \"\"\"\n    print(\"\\nğŸ“¦ STEP 4: Checking order status...\")\n    \n    order_data = state[\"order_data\"]\n    customer_data = state[\"customer_data\"]\n    \n    # Prepare status information\n    status_info = {\n        \"order_id\": order_data[\"order_id\"],\n        \"customer_name\": customer_data[\"name\"],\n        \"status\": order_data[\"status\"],\n        \"items\": order_data[\"items\"],\n        \"total_amount\": order_data[\"total_amount\"],\n        \"order_date\": order_data[\"order_date\"],\n        \"estimated_delivery\": order_data[\"estimated_delivery\"],\n        \"shipping_address\": order_data[\"shipping_address\"],\n        \"tracking_number\": order_data.get(\"tracking_number\"),\n        \"carrier\": order_data[\"carrier\"],\n        \"last_update\": order_data[\"last_update\"],\n        \"actual_delivery\": order_data.get(\"actual_delivery\")\n    }\n    \n    print(f\"   âœ… Status retrieved: {status_info['status']}\")\n    print(f\"   Latest update: {status_info['last_update']}\")\n    \n    return {\"status_info\": status_info}\n\nprint(\"âœ… Check status node defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Prompt Chain Step 5: Generate Success Response\n\nThis is the final step that takes all gathered information and generates a friendly, conversational response for the customer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_success_response_node(state: OrderEnquiryState) -> Dict:\n    \"\"\"\n    Step 5: Generate personalized success response\n    \"\"\"\n    print(\"\\nâœ¨ STEP 5: Generating personalized response...\")\n    \n    status_info = state[\"status_info\"]\n    \n    system_prompt = \"\"\"You are a friendly customer service representative. \nGenerate a warm, personalized response about the customer's order.\n\nGuidelines:\n- Address customer by name\n- Clearly state order status\n- Include delivery information\n- Add tracking details if available\n- Be conversational and helpful\n- Keep it concise (3-4 sentences)\n\"\"\"\n    \n    user_prompt = f\"\"\"Create a response for this order:\n\n{json.dumps(status_info, indent=2)}\n\"\"\"\n    \n    try:\n        # Use LangChain ChatOpenAI\n        messages = [\n            SystemMessage(content=system_prompt),\n            HumanMessage(content=user_prompt)\n        ]\n        \n        response = llm.invoke(messages)\n        final_response = response.content\n        \n        print(f\"   âœ… Response generated ({len(final_response)} chars)\")\n        \n        return {\"final_response\": final_response}\n        \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        return {\"final_response\": f\"Your order #{status_info['order_id']} is {status_info['status']}.\"}\n\nprint(\"âœ… Generate success response node defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Routing Logic After Validation\n",
    "\n",
    "Define conditional routing after validation. If order not found (error response already generated), go to END. Otherwise, continue to check status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_validation(state: OrderEnquiryState) -> str:\n",
    "    \"\"\"\n",
    "    Route after validation node\n",
    "    - If order found: continue to check_status\n",
    "    - If order not found: go directly to END (error response already generated)\n",
    "    \"\"\"\n",
    "    if state.get(\"order_found\", False):\n",
    "        print(\"\\nğŸ”€ ROUTING: Validation passed â†’ Check status\")\n",
    "        return \"check_status\"\n",
    "    else:\n",
    "        print(\"\\nğŸ”€ ROUTING: Validation failed â†’ END (error response set)\")\n",
    "        return \"END\"\n",
    "\n",
    "print(\"âœ… Routing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Build the Prompt Chain Graph\n",
    "\n",
    "Construct the LangGraph workflow with a validation node that stops the flow when order is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_chain_graph():\n",
    "    \"\"\"Build the prompt chaining workflow with validation node\"\"\"\n",
    "    \n",
    "    # Initialize graph builder\n",
    "    builder = StateGraph(OrderEnquiryState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    builder.add_node(\"extract_info\", extract_info_node)\n",
    "    builder.add_node(\"database_lookup\", database_lookup_node)\n",
    "    builder.add_node(\"validation\", validation_node)  # New validation node\n",
    "    builder.add_node(\"check_status\", check_status_node)\n",
    "    builder.add_node(\"generate_success\", generate_success_response_node)\n",
    "    \n",
    "    # Define linear chain\n",
    "    builder.add_edge(START, \"extract_info\")\n",
    "    builder.add_edge(\"extract_info\", \"database_lookup\")\n",
    "    builder.add_edge(\"database_lookup\", \"validation\")\n",
    "    \n",
    "    # Conditional routing after validation\n",
    "    builder.add_conditional_edges(\n",
    "        \"validation\",\n",
    "        route_after_validation,\n",
    "        {\n",
    "            \"check_status\": \"check_status\",\n",
    "            \"END\": END  # Direct to END when validation fails\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Success path\n",
    "    builder.add_edge(\"check_status\", \"generate_success\")\n",
    "    builder.add_edge(\"generate_success\", END)\n",
    "    \n",
    "    # Compile graph\n",
    "    graph = builder.compile()\n",
    "    \n",
    "    print(\"âœ… Prompt chain graph compiled with validation node\")\n",
    "    return graph\n",
    "\n",
    "# Create the graph\n",
    "graph = create_prompt_chain_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize the Workflow\n",
    "\n",
    "Display the graph structure showing the prompt chaining flow with conditional branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"Graph visualization:\")\n",
    "    print(\"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚     START       â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  extract_info   â”‚  â† Step 1: Extract order ID from query\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚database_lookup  â”‚  â† Step 2: Query database\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   validation    â”‚  â† Step 3: Validate & handle errors\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â†“\n",
    "         [Found?]\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
    "     Yes            No\n",
    "      â†“              â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    (END with error)\n",
    "    â”‚ check  â”‚\n",
    "    â”‚ status â”‚  â† Step 4: Get status info\n",
    "    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚generateâ”‚\n",
    "    â”‚success â”‚  â† Step 5: Create response\n",
    "    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚       END       â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test the Prompt Chain\n",
    "\n",
    "Run the complete workflow with different test queries to see the prompt chain in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_order_enquiry(query: str):\n",
    "    \"\"\"Execute the prompt chain with a user query\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"ğŸ’¬ USER QUERY: {query}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Invoke the graph\n",
    "        result = graph.invoke({\n",
    "            \"user_query\": query,\n",
    "            \"extracted_info\": None,\n",
    "            \"order_data\": None,\n",
    "            \"customer_data\": None,\n",
    "            \"order_found\": False,\n",
    "            \"status_info\": None,\n",
    "            \"final_response\": \"\"\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ¤– ASSISTANT RESPONSE:\")\n",
    "        print(\"=\"*70)\n",
    "        print(result[\"final_response\"])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… Test function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: Successful Order Lookup (In Transit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_order_enquiry(\"When will I get my order 101 delivered?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Successful Order Lookup (Delivered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_order_enquiry(\"Has my order #102 arrived yet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: Order Not Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_order_enquiry(\"Where is my order 999?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 4: Processing Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_order_enquiry(\"What's the status of order 103?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Interactive Testing Widget\n",
    "\n",
    "Create an interactive interface to test the prompt chain with custom queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create widgets\n",
    "    query_input = widgets.Textarea(\n",
    "        value='',\n",
    "        placeholder='Enter your order enquiry here... (e.g., \"Where is my order 101?\")',\n",
    "        description='Your Query:',\n",
    "        layout=widgets.Layout(width='600px', height='80px'),\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    # Predefined test queries\n",
    "    test_queries_dropdown = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('-- Select a test query --', ''),\n",
    "            ('Order 101 - In Transit', 'When will I get my order 101 delivered?'),\n",
    "            ('Order 102 - Delivered', 'Has my order #102 arrived?'),\n",
    "            ('Order 103 - Processing', \"What's the status of order 103?\"),\n",
    "            ('Order Not Found', 'Where is my order 999?'),\n",
    "            ('Casual Query', \"Hi, I'm Sarah checking on order 101\"),\n",
    "        ],\n",
    "        description='Examples:',\n",
    "        layout=widgets.Layout(width='600px'),\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='Run Query',\n",
    "        button_style='primary',\n",
    "        icon='play'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_dropdown_change(change):\n",
    "        if change['new']:\n",
    "            query_input.value = change['new']\n",
    "    \n",
    "    def on_run_click(button):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            if query_input.value.strip():\n",
    "                run_order_enquiry(query_input.value)\n",
    "            else:\n",
    "                print(\"âš ï¸  Please enter a query\")\n",
    "    \n",
    "    test_queries_dropdown.observe(on_dropdown_change, names='value')\n",
    "    run_button.on_click(on_run_click)\n",
    "    \n",
    "    # Display interface\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h3>ğŸ§ª Interactive Prompt Chain Tester</h3>\"),\n",
    "        test_queries_dropdown,\n",
    "        query_input,\n",
    "        run_button,\n",
    "        output_area\n",
    "    ]))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  ipywidgets not available. Use run_order_enquiry() function instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Key Learnings: Prompt Chaining Pattern\n",
    "\n",
    "### âœ… **When to Use Prompt Chaining**\n",
    "- Tasks with **predetermined steps** in a fixed sequence\n",
    "- When you need **predictable, observable workflows**\n",
    "- For **structured information extraction and transformation**\n",
    "- When debugging and testing discrete steps is important\n",
    "\n",
    "### âš ï¸ **When NOT to Use Prompt Chaining**\n",
    "- Tasks requiring **dynamic tool selection**\n",
    "- Problems that need **adaptive decision-making**\n",
    "- Scenarios where the solution path is **unpredictable**\n",
    "- Complex multi-step reasoning that requires iteration\n",
    "\n",
    "### ğŸ“Š **Pattern Comparison**\n",
    "\n",
    "| Aspect | Prompt Chaining | Agentic Pattern |\n",
    "|--------|----------------|------------------|\n",
    "| **Flow** | Fixed sequence | Dynamic loops |\n",
    "| **Decision Making** | Predetermined | Self-directed |\n",
    "| **Predictability** | High | Variable |\n",
    "| **Observability** | Excellent | Moderate |\n",
    "| **Flexibility** | Low | High |\n",
    "| **Use Case** | Structured tasks | Open-ended problems |\n",
    "\n",
    "### ğŸ’¡ **Best Practices**\n",
    "1. **Store raw data in state** - Format in nodes as needed\n",
    "2. **One responsibility per node** - Keep nodes focused and testable\n",
    "3. **Clear state updates** - Each node should update specific state fields\n",
    "4. **Graceful error handling** - Provide clear error paths and messages\n",
    "5. **Detailed logging** - Track execution flow for debugging\n",
    "\n",
    "### ğŸ¯ **This Implementation Demonstrated**\n",
    "- âœ… Sequential LLM calls with context passing\n",
    "- âœ… Conditional branching based on intermediate results\n",
    "- âœ… Database integration within a prompt chain\n",
    "- âœ… Error handling with alternative paths\n",
    "- âœ… Personalized response generation\n",
    "- âœ… State management with typed dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Available Test Data Reference\n",
    "\n",
    "### ğŸ“‹ **Valid Order IDs for Testing**\n",
    "Use any of these order IDs to test the workflow:\n",
    "- **101-118** - Various orders with different statuses (in_transit, delivered, processing, cancelled)\n",
    "- Try: 101, 102, 103, 105, 106, 109, 111, 114, 117\n",
    "\n",
    "### ğŸ“ **Sample Queries to Try**\n",
    "```\n",
    "\"When will I get my order 101 delivered?\"\n",
    "\"Has my order #102 arrived yet?\"\n",
    "\"What's the status of order 103?\"\n",
    "\"Where is my order 999?\" (not found)\n",
    "\"I'm Sarah checking on order 101\"\n",
    "\"Track my order 105 please\"\n",
    "\"Order 106 status?\" (cancelled order)\n",
    "\"What's happening with my order 114?\"\n",
    "```\n",
    "\n",
    "### ğŸ“Š **Order Status Types**\n",
    "- **in_transit**: Orders currently being shipped\n",
    "- **delivered**: Successfully delivered orders\n",
    "- **processing**: Orders being prepared\n",
    "- **cancelled**: Cancelled orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Resources\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Thinking in LangGraph](https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph)\n",
    "- [Workflows vs Agents](https://docs.langchain.com/oss/python/langgraph/workflows-agents)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates a production-ready prompt chaining workflow for automated customer service.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}